{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all our favorite packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rushd as rd\n",
    "import scipy as sp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the path to the SharePoint using `datadir`\n",
    "To enable reproducible analysis, Python notebooks/code should be able to run from locations other than your personal computer. However, because our data is stored in the cloud, the location of the data files will differ across users. Namely, accessing the lab SharePoint requires an absolute path specific to your computer. \n",
    "\n",
    "To get around this issue, `rushd` facilitates one-time user specification of the path to this data directory, or `datadir`, that remains constant for a particular location (i.e., a github repository stored on your computer). This way, you include the absolute path to the data-containing directory outside of your Python notebook, meaning that cloning the repository / running the code elsewhere requires only one change to properly run the analyses.\n",
    "\n",
    "To specify a `datadir`, write the absolute path to the data-containing directory in a text file called `datadir.txt`. Use the highest-level directory you anticipate needing, i.e., the main SharePoint (not one of the subfolders). Include only this line in the file, and do not enclose the path in quotes. For instance, the path to the SharePoint on my computer is: \n",
    "\n",
    "`/Users/kaseylove/Massachusetts Institute of Technology/GallowayLab - Documents`\n",
    "\n",
    "so my `datadir.txt` file contains just this line. The file is stored in the root directory of my git repository.\n",
    "\n",
    "Then, to access data in this directory, simply use `rd.datadir` as the path to this folder. To access subfolders, normal Path-type operations apply, e.g., `rd.datadir/'subfolder'`. Additional usage examples are below. \n",
    "\n",
    "The path to root directory of your repository is also conveniently accessible via `rushd` using `rd.rootdir` (no text file required). This is helpful for specifying output paths when saving figures/files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add well metadata using a `.yaml` file\n",
    "\n",
    "The default Attune and FlowJo filenames for samples comprise the well number without other condition information. To add this metadata (e.g., plasmids, small molecules), create a `.yaml` file to map conditions to wells. See this quick tutorial for creating these files: https://learnxinyminutes.com/docs/yaml/. Here is an example format:\n",
    "\n",
    "```\n",
    "metadata:\n",
    "  group1:\n",
    "    - valueA: A1-A12\n",
    "    - valueB: B1-B12\n",
    "  group2:\n",
    "    - 0: A1-H1\n",
    "    - 1: A2-H2\n",
    "```\n",
    "\n",
    "where each group will become a column in the final DataFrame, with column names `group1` and `group2` and values `valueA`, `valueB`, `0`, or `1`.\n",
    "\n",
    "Specifically, here are the contents of an example `.yaml` file ([link](https://mitprod.sharepoint.com/:u:/s/GallowayLab/EaYvEuC2pFZAthDMbKNZvyQBtWvaCUpX1tWi6wEhoGeLHw?e=PXO9n7)). In this experiment, a reporter plasmid ('construct') was transfected with different transcriptional activator plasmids ('activator')\n",
    "and treated with different small-molecule inducers ('inducer'). (Here, 'NT' refers to the non-transfected/ (or untransfected) condition.) Each experimental condition is defined by a unique combination\n",
    "of construct, activator, and inducer, and there are multiple wells per condition.\n",
    "\n",
    "```\n",
    "metadata: \n",
    "  construct:\n",
    "    - pGEEC570: A1-B4, E1-F4\n",
    "    - pGEEC525: C1-C4, G1-G4\n",
    "    - pGEEC526: D1-D4, H1-H4\n",
    "    - pGEEC549: A5-A8\n",
    "    - pGEEC516: B5-B8\n",
    "    - pGEEC577: C5-C8\n",
    "    - NT: D5-D8\n",
    "  activator:\n",
    "    - pGEEC572: A1-A4, E1-E4\n",
    "    - pGEEC574: B1-B4, F1-F4\n",
    "    - pGEEC576: C1-C4, G1-G4\n",
    "    - pGEEC578: D1-D4, H1-H4\n",
    "    - NT: D5-D8\n",
    "    - none: A5-C8\n",
    "  inducer:\n",
    "    - DMSO: A1-A4, C1-C4\n",
    "    - EtOH: B1-B4\n",
    "    - none: D1-D4, A5-D8\n",
    "    - GZV: E1-E4\n",
    "    - 4OHT: F1-F4\n",
    "    - Rap: G1-G4\n",
    "    - dox: H1-H4\n",
    "```\n",
    "\n",
    "Save this file in the same folder as (or near) your raw data.\n",
    "\n",
    "To double check that the `.yaml` file correctly specifies conditions, you can display the associated plate map using `rushd`.\n",
    "See the example below and https://gallowaylabmit.github.io/rushd/en/main/tutorial/plot_well_metadata.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plate layout example\n",
    "\n",
    "View the layout of conditions on an example plate.\n",
    "There will be a separate plate map for each metadata group.\n",
    "Note: If multiple metadata values for a particular group map\n",
    "to the same well, the name will be \"valueA.valueB\". (Usually, you\n",
    "don't want this.) Use this visualization to identify any \n",
    "errors in your yaml file!\n",
    "'''\n",
    "yaml_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.09.21_exp114'/'export_comp'/'wells.yaml'\n",
    "rd.plot.plot_well_metadata(yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate your data in FlowJo\n",
    "\n",
    "To obtain loadable data, you need to pre-process it in FlowJo. Running your samples on the Attune (or any flow cytometer) produces `.fcs` files with every event captured during the run, one file per well of the plate (or per tube). Load these files into FlowJo, and gate the events corresponding to single cells. Typically, this involves two steps, looking at the forward and side scatter values (FSC and SSC). Then, export this single cell population (often named \"singlets\") for each sample as `.csv` files (again, one per well).\n",
    "\n",
    "Now you are ready to load your data. As a summary, you should have the following:\n",
    "\n",
    "- `datadir.txt` with the path to the SharePoint, located in the root directory of your repository\n",
    "- A `.yaml` file with well-level metadata for your experiment, typcially located in the same folder as your data in the SharePoint\n",
    "- A folder containing your pre-processed data (events gated on single cells) as `.csv` files, located in the SharePoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: Use `rushd` package to import `.csv` data into a Pandas DataFrame with associated metadata\n",
    "The function loads data from all wells into a single DataFrame object, which is nicely compatible with Seaborn plotting functions (see description of 'long-form' data here: https://seaborn.pydata.org/tutorial/data_structure.html and Pandas documentation here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "Each row is a single cell, and columns are the data associated with that cell. This includes all the measurement channels on the Attune (e.g., FSC-H, VL1-A, mCherry-A, Time -- it will use the names that you set for the channels on the Attune software) as well as the information from the filename (e.g., well, FlowJo population) and your `.yaml` file (e.g., plasmid/construct, small molecule conditions, replicate number).\n",
    "\n",
    "See examples below. Feel free to take a look at the filenames and `.yaml` files in the indicated folder for reference. If your `datadir` is set up properly, you should be able to run these cells and see the data loaded into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 1: Single plate\n",
    "\n",
    "This uses the default filename from exporting in FlowJo, namely `export_{well}_{population}.csv`.\n",
    "Columns labeled 'well' and 'population' are added based on the filename.\n",
    "Here, for instance, the file `export_A1_singlets.csv` is added with 'A1' in the 'wells' column \n",
    "and 'singlets' in the 'population' column.\n",
    "'''\n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.12.04_exp092.3'/'export'\n",
    "yaml_path = data_path/'wells.yaml'\n",
    "data = rd.flow.load_csv_with_metadata(data_path, yaml_path)\n",
    "display(data)\n",
    "\n",
    "# Print the column names to see all the Attune channels\n",
    "display(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 2: Load selected columns\n",
    "\n",
    "This example loads the same data as above, but only the two channels we care about,\n",
    "specified via the 'columns' argument. This saves time/space by not storing values \n",
    "for irrelevant Attune channels.\n",
    "'''\n",
    "channel_list = ['mRuby2-A','mGL-A']\n",
    "data = rd.flow.load_csv_with_metadata(data_path, yaml_path, columns=channel_list)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 3: Multiple plates\n",
    "\n",
    "This example loads four plates. Files are named with the default FlowJo naming as above, and\n",
    "the data for each plate are stored in separate folders. These data are then loaded into a single\n",
    "DataFrame with extra metadata specifying the cell type in each plate.\n",
    "'''\n",
    "base_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.02.07_exp77.3'/'export'\n",
    "\n",
    "plates = pd.DataFrame({\n",
    "    'data_path': [base_path/f'plate{n}' for n in range(1,5)],\n",
    "    'yaml_path': [base_path/'exp77.3_plate1_wells.yaml', base_path/'exp77.3_plate2_wells.yaml',]*2,\n",
    "    'cell': ['MEF', 'MEF', '293T', '293T'],\n",
    "})\n",
    "\n",
    "data2 = rd.flow.load_groups_with_metadata(plates, columns=['mRuby2-A','mGL-A'])\n",
    "display(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to a local cache\n",
    "\n",
    "Loading data from the SharePoint can sometimes take several minutes since each file must be downloaded from the server. To speed up this step for future analyses, you can download the data once from the SharePoint and save the relevant components locally on your computer. It is useful to combine this step with the original loading one.\n",
    "\n",
    "A convenient place for this cache is in a folder in the analysis repo. You may wish to save this file in the same place as any plots you generate during analysis, for instance a folder for the experiment within the folder `output` in your git repo. Make sure to add the `output` folder to `.gitignore` so that git doesn't track these large files.\n",
    "\n",
    "`rushd` also provides the function `outfile` that creates a `.yaml` file containing metadata associated with the output (e.g., git version). It will also create any directories in the output path that don't already exist, which is convenient. You can wrap any output path in `outfile`, including your data cache and any plots you generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Local data cache\n",
    "\n",
    "Save the data as a `.gzip` file in a folder in the 'output'\n",
    "folder in your git repo. The if-else structure here is \n",
    "convenient because you can re-run this cell of the notebook\n",
    "whether or not the cache already exists.\n",
    "'''\n",
    "cache_path = rd.rootdir/'output'/'exp092.3'/'data.gzip'\n",
    "\n",
    "# Specify the data to load\n",
    "channel_list = ['mRuby2-A','mGL-A']\n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.12.04_exp092.3'/'export'\n",
    "yaml_path = data_path/'wells.yaml'\n",
    "\n",
    "data3 = pd.DataFrame()\n",
    "\n",
    "# If cache exists, load data from cache\n",
    "if cache_path.is_file(): \n",
    "    data3 = pd.read_parquet(cache_path)\n",
    "\n",
    "# Otherwise, load from SharePoint and create cache\n",
    "else: \n",
    "    data3 = rd.flow.load_csv_with_metadata(data_path, yaml_path, columns=channel_list)\n",
    "    data3.to_parquet(rd.outfile(cache_path))\n",
    "    \n",
    "display(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional condition-level metadata\n",
    "\n",
    "Sometimes conditions, such as plasmids, vary across multiple dimensions (promoter, gene, syntax, etc.) that would be helpful to add as metadata. This can be cumbersome to add to your `.yaml` file, especially if you have multiple plasmids per condition or reuse plasmids across experiments. Therefore, it can be helpful to include only the plasmid name in the `.yaml` file mapping conditions to wells and also to create a single spreadsheet with plasmid metadata. You can save this in the git repo directly, or in a project folder in the SharePoint.\n",
    "\n",
    "For example, a spreadsheet for TANGLES constructs could look like this:\n",
    "\n",
    "| plasmid | upstream_gene | downstream_gene | spacer | syntax             |\n",
    "| ------- | ------------- | --------------- | ------ | ------------------ |\n",
    "| pTA001  | tagBFP        | mRuby2          | 1x     | downstream_tandem  |\n",
    "| pTA002  | tagBFP        | mRuby2          | 1x     | divergent          |\n",
    "| pTA003  | tagBFP        | mRuby2          | 1x     | convergent         |\n",
    "| pTA004  | mRuby2        | tagBFP          | 1x     | upstream_tandem    |\n",
    "\n",
    "You can then load this spreadsheet as a DataFrame and merge it with your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 1: Column names match\n",
    "\n",
    "If the name of the first column in the metadata file matches a column name in your data, \n",
    "you can merge directly using the 'on' argument. Here, the plasmid IDs are in the\n",
    "'construct' column of both the data and the metadata.\n",
    "'''\n",
    "# Load metadata from an Excel file into a DataFrame\n",
    "metadata_path = rd.datadir/'projects'/'miR-iFFL'/'plasmids'/'construct-metadata.xlsx'\n",
    "metadata = pd.read_excel(metadata_path)\n",
    "\n",
    "# Add metadata to data\n",
    "data = data.merge(metadata, how='left', on='construct')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 2: Multiple columns with plasmids\n",
    "\n",
    "If you need to add metadata for two columns in your data, you can repeatedly call `merge` \n",
    "on each. See the pandas documentation for more information. This example adds plasmid \n",
    "metadata to the 'construct' column (the reporter plasmid) and the 'activator' \n",
    "column (co-transfected activator plasmid). Notice that the activator metadata contain a\n",
    "suffix to differentiate them.\n",
    "'''\n",
    "# Load new dataset\n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.07.16_exp099'/'export_comp'\n",
    "yaml_path = data_path/'wells.yaml'\n",
    "cache_path = rd.rootdir/'output'/'KL_exp099'/'data.gzip'\n",
    "\n",
    "data4 = pd.DataFrame()\n",
    "if cache_path.is_file(): data4 = pd.read_parquet(cache_path)\n",
    "else: \n",
    "    channel_list = ['mRuby2-A','AF514-A','tagBFP-A']\n",
    "    data4 = rd.flow.load_csv_with_metadata(data_path, yaml_path, columns=channel_list)\n",
    "    data4.to_parquet(rd.outfile(cache_path))\n",
    "\n",
    "# Add metadata\n",
    "metadata4 = pd.read_excel(rd.datadir/'projects'/'geec'/'construct-metadata_KL.xlsx')\n",
    "data4 = data4.merge(metadata4, how='left', on='construct')\n",
    "data4 = data4.merge(metadata4, how='left', right_on='construct', left_on='activator', suffixes=(None,'_activator'))\n",
    "display(data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have your data loaded with useful metadata! It's time to see what the data shows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set plotting defaults\n",
    "\n",
    "To explore trends in your data, you'll make a bunch of plots. To make the plots look nicer, you can set some basic defaults for font size, line width, etc. (This is much more important for polished figures, but starting with decent plots now will make even quick slides easier to understand.) \n",
    "\n",
    "This is also a good time to define a color palette for your conditions. See Seaborn's [Choosing color palettes](https://seaborn.pydata.org/tutorial/color_palettes.html) for suggestions, or try a palette from someone else in lab. You can specify colors using the matplotlib named colors, hex codes, or a few other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seaborn style (applies to entire notebook)\n",
    "\n",
    "The 'talk' context sets font sizes, etc. appropriate for a presentation.\n",
    "(Other options include 'notebook', 'paper', and 'poster'.)\n",
    "This also sets the font to Helvetica Neue, but you can change this to whatever \n",
    "you prefer. See Seaborn/matplotlib documentation for other parameters.\n",
    "'''\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('talk', rc={'font.family': 'sans-serif', 'font.sans-serif':['Helvetica Neue']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a color palette\n",
    "\n",
    "Use a dictionary to map categorical condition values to colors.\n",
    "Here, colors are assigned to different gene syntaxes. Additionally,\n",
    "this modified form of the viridis palette (yellow -> purple) is \n",
    "good for continuous values (e.g., small molecule amounts).\n",
    "'''\n",
    "# Categorical palette\n",
    "palette = {\n",
    "    'tandem_reporter_upstream': '#225A9B',\n",
    "    'tandem_reporter_downstream': '#19D2BF',\n",
    "    'convergent': '#FFB133',\n",
    "    'divergent': '#FE484E',\n",
    "}\n",
    "\n",
    "# Continuous palette\n",
    "no_yellow_viridis = matplotlib.colors.ListedColormap(matplotlib.colormaps['viridis'](np.linspace(0, 0.85, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove cells with negative channel values\n",
    "\n",
    "Negative values from the Attune are essentially \"off the chart\" and represent non-expressing cells. There aren't usually too many of them, and it is safe to simply exclude them. This makes it simpler to plot the data, which is log-distributed. (Note that some plotting functions or calculations will throw an error when run on data with non-positive values.)\n",
    "\n",
    "To remove these cells, you can use a \"mask\", finding the rows that satisfy some True/False statement and then reassigning `data` to this value. To remove cells with negative channel measurements, the statement will specify that the value in each channel column be greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove cells with negative channel values\n",
    "\n",
    "The 'channel_list' should contain all the Attune channels you're \n",
    "interested in (e.g., mGL-A) that could have non-positive values.\n",
    "Here, we filter all three datasets that were loaded above.\n",
    "'''\n",
    "for c in ['mRuby2-A','mGL-A']:\n",
    "    data = data[data[c] > 0]\n",
    "    data2 = data2[data2[c] > 0]\n",
    "\n",
    "channel_list = ['mRuby2-A','AF514-A','tagBFP-A']\n",
    "for c in channel_list:\n",
    "    data3 = data3[data3[c] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms & joint distributions\n",
    "\n",
    "The first thing you'll likely want to do is visualize distributions of expression across relevant channels for various conditions. The easiest way to plot multiple conditions at once is by using Seaborn's FacetGrid and related functions: https://seaborn.pydata.org/tutorial/axis_grids.html With these functions, you shouldn't have to define each axis yourself!\n",
    "\n",
    "Seaborn plotting functions come in several types:\n",
    "\n",
    "1. **Functions that generate a single axis** (return a matplotlib `Axes` object). You can facet the data on up to two columns (e.g., one category on the x-axis and the other as the color). This includes `scatterplot`, `kdeplot`, etc.\n",
    "\n",
    "2. **Functions that generate a grid of axes** (return a Seaborn `FacetGrid` object). You can facet the data on up to four columns (e.g., x-axis, color, row in the grid, column in the grid). These functions typically have several different plot type options, which you can specify with the `kind` argument. For instance, `catplot` includes 'strip', 'bar', 'violin' and other plot types with one categorical axis, while `displot` includes 'kde' and 'hist' plots to show 1D or 2D distributions.\n",
    "\n",
    "You probably want to begin by using `kdeplot` to plot 1D or 2D distributions: https://seaborn.pydata.org/generated/seaborn.kdeplot.html Note that 2D kdeplots may take several minutes to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 1: Plot 1D kdeplot\n",
    "\n",
    "For kdeplots, be sure to use a log scale and normalize the area under the\n",
    "curve within conditions rather than across them (no \"common normalization\",\n",
    "i.e., 'common_norm=False').\n",
    "You can manually adjust the placement of the legend to move it outside of\n",
    "the plot area.\n",
    "'''\n",
    "ax = sns.kdeplot(\n",
    "        data=data, x='mGL-A', hue='construct',  # specify the DataFrame, x-axis, and color column\n",
    "        log_scale=True, common_norm=False       # additional arguments for kdeplots \n",
    "    )\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 2: Plot kdeplots in a grid\n",
    "\n",
    "Use the corresponding grid functions to facet your data along \n",
    "rows/columns of a grid. \n",
    "Here, we subset the data to only plot conditions where the \n",
    "'ts_kind' column has the value 'NT' (OL circuit) or 'T' (CL circuit).\n",
    "'''\n",
    "# Subset data into a DataFrame for plotting\n",
    "plot_df = data[data.ts_kind.isin(['NT','T'])]\n",
    "\n",
    "# Plot in a grid\n",
    "g = sns.displot(\n",
    "        data=plot_df, x='mGL-A', hue='ts_num',  # same arguments as above\n",
    "        palette=no_yellow_viridis,              # specify the color palette (dictionary mapping values of the hue column to colors)\n",
    "        col='ts_kind', kind='kde',              # additional arguments for displot: facet grid columns on 'ts_kind' and plot a kde plot\n",
    "        log_scale=True, common_norm=False,      # same kde-specific arguments as above\n",
    "    )\n",
    "\n",
    "# Loop over the axes to add the untransfected condition for comparison\n",
    "for ax in g.axes_dict.values():\n",
    "    sns.kdeplot(data=data[data.construct=='UT'], x='mGL-A', color='black', ls=':', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 2: Plot 2D kdeplot in a grid\n",
    "\n",
    "This plot uses the same plotting function as above.\n",
    "It can help to downsample your data to fewer cells per condition\n",
    "so that initial plots generate faster (remove this for \n",
    "final figures).\n",
    "'''\n",
    "# Subset data to remove conditions where the activator is 'na'\n",
    "plot_df = data4[(data4.gene_activator!='na') & (data4.promoter_activator!='na')]\n",
    "\n",
    "# Downsample to 1000 cells per condition, where conditions are defined\n",
    "#  by the 'construct' and 'activator' columns\n",
    "plot_df = plot_df.groupby(['construct','activator']).sample(1000)\n",
    "\n",
    "# Plot\n",
    "g = sns.displot(\n",
    "        data=plot_df, x='AF514-A', y='mRuby2-A', hue='inducer',\n",
    "        row='gene_activator', col='promoter_activator',\n",
    "        log_scale=True, common_norm=False, kind='kde', \n",
    "        facet_kws=dict(margin_titles=True)                      # additional facet argument to move row titles to the right\n",
    "    )\n",
    "\n",
    "# Adjust plots\n",
    "g.set(xlim=(1e1,1e5), ylim=(1e0,1e5)) # 'set' includes many more parameters, see matplotlib documentation\n",
    "g.set_titles(row_template='{row_name}', col_template='activator promoter: {col_name}')\n",
    "g.refline(y=2e2, color='black', ls='-', zorder=0) # add a horizontal reference line to each axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate expressing cells\n",
    "\n",
    "Depending on your experiment, you might want to analyze only a fraction of the population. For transfections, we typically only care about transfected cells, or those expressing the transfection marker (co-delivered fluorescent protein). You can manually eyeball this threshold (or gate), or you can set it based some high percentile of the untransfected cells. It can be helpful to make a new DataFrame with only these cells. Be sure to exclude any conditions that lack the transfection marker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 1: Choose a threshold manually\n",
    "'''\n",
    "gate = 3e2  # manual gate\n",
    "data_gated = data[(data['mGL-A']>gate) & (data.construct!='UT')].copy()\n",
    "\n",
    "ax = sns.kdeplot(data=data, x='mGL-A', hue='construct', \n",
    "                 log_scale=True, common_norm=False)\n",
    "ax.axvline(gate, color='black', zorder=0)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 2: Gate based on the untransfected population\n",
    "\n",
    "Here, we use the 99.9th percentile of the untransfected population (UT)\n",
    "as the gate, meaning that only 0.1% of untransfected cells are (mis)labeled\n",
    "as expressing. This gate is for the mGL-A channel, which represents the\n",
    "transfection marker.\n",
    "'''\n",
    "# Compute the gate\n",
    "gate = data.loc[data.construct=='UT', 'mGL-A'].quantile(0.999)\n",
    "display(gate)\n",
    "\n",
    "data_gated = data[(data['mGL-A']>gate) & (data.construct!='UT')].copy()\n",
    "\n",
    "ax = sns.kdeplot(data=data, x='mGL-A', hue='construct', \n",
    "                 log_scale=True, common_norm=False)\n",
    "ax.axvline(gate, color='black', zorder=0)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 3: Use different gates for different conditions\n",
    "\n",
    "We want to use different gates for the MEFs and 293T cells since they have\n",
    "different autofluorescence profiles. We'll define gates based on the \n",
    "uninfected ('UI') populations for each cell type.\n",
    "'''\n",
    "# Define a function to choose the gate and return the gated population\n",
    "#  Input: (group of a) DataFrame --> Returns: subset of that DataFrame group\n",
    "def gate_data(df):\n",
    "    gate = df.loc[df.construct=='UI', 'mGL-A'].quantile(0.999)\n",
    "    display(gate)\n",
    "    return df[(df['mGL-A']>gate) & (df.construct!='UI')]\n",
    "\n",
    "# Gate data\n",
    "data2_gated = data2.groupby(['cell'])[data2.columns].apply(gate_data).reset_index(drop=True)\n",
    "display(data2_gated)\n",
    "\n",
    "# Plot uninfected populations to visualize autofluorescence profiles\n",
    "plot_df = data2[data2.construct=='UI'].groupby('cell').sample(1000)\n",
    "cell_palette = {'293T': 'teal', 'MEF': 'orange'}\n",
    "g = sns.displot(data=plot_df, x='mGL-A', y='mRuby2-A', col='cell', \n",
    "                hue='cell', palette=cell_palette,\n",
    "                kind='kde', log_scale=True)\n",
    "g.refline(x=257, ls='-', color=cell_palette['293T'], zorder=0)\n",
    "g.refline(x=345, ls='-', color=cell_palette['MEF'], zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate summary statistics\n",
    "\n",
    "Now that you've explored the distributions for each condition, you probably want to quantify trends. Calculating summary statistics (mean, standard deviation, etc.) is straightforward and quick with Pandas functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate summary statistics for multiple channels at once\n",
    "\n",
    "We want to compute the geometric mean (gmean) and standard \n",
    "deviation for both the output and marker channels. Conveniently,\n",
    "Pandas can do all four of these calculations at once. The \n",
    "DataFrame resulting from this computation ('agg' function) has\n",
    "both a multi-level index (from 'groupby') and multi-level \n",
    "column names. We can collapse the index into columns with 'reset_index',\n",
    "but flattening the column names is more complicated.\n",
    "'''\n",
    "# Compute geometric mean (gmean) and standard deviation on two relevant channels\n",
    "channel_list = ['mGL-A','mRuby2-A']\n",
    "stats = data_gated.groupby('construct')[channel_list].agg([sp.stats.gmean, np.std]).reset_index().dropna()\n",
    "\n",
    "# Flatten the multi-level column names to '{level1}_{level2}' \n",
    "#  without leaving a hanging '_' for single-level columns\n",
    "stats.columns = ['_'.join(c).rstrip('_') for c in stats.columns.to_flat_index()]\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "A function for computing summary statistics\n",
    "\n",
    "The computation above can be summarized into a convenient function. \n",
    "Maybe soon we'll add it to rushd, but for now feel free to use & modify\n",
    "it yourself!\n",
    "\n",
    "Inputs\n",
    "  df: a DataFrame with your data\n",
    "  by: a list of columns used to group the data for summarizing\n",
    "  columns: a list of columns to summarize (i.e., channels)\n",
    "  stats: a list of functions to use to summarize (i.e., stats)\n",
    "Returns\n",
    "  stats: a new DataFrame, where each row is a unique condition defined\n",
    "    by 'by' and the number of columns = len(by) + len(columns) x len(stat_list)\n",
    "'''\n",
    "def summarize(df, by, columns, stat_list):\n",
    "    stats = df.groupby(by)[columns].agg(stat_list).reset_index().dropna()\n",
    "    stats.columns = ['_'.join(c).rstrip('_') for c in stats.columns.to_flat_index()]\n",
    "    return stats\n",
    "\n",
    "# Calculate statistics on gated data\n",
    "stats = summarize(data_gated, 'construct', ['mGL-A','mRuby2-A'], [sp.stats.gmean, np.std])\n",
    "\n",
    "# Add construct metadata to stats DataFrame too\n",
    "stats = stats.merge(metadata, how='left', on='construct')\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot summary statistics\n",
    "\n",
    "There are many ways to plot summary statistics (box plot, scatter plot, bar plot, etc.) and several ways to display the variability between measurements (error bars, shading, etc.). Choose your favorite representation!\n",
    "\n",
    "One recommendation: do not use bar plots for values without a relevant zero, and always display the zero on the axis. This ensures the sizes of the bars accurately reflect the relative values they represent. For example, bar plots are effective for displaying percentages (e.g., reprogramming purity) but not geometric mean fluorescence values (which are log-distributed and typically much higher than 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 1: Dataset 1\n",
    "\n",
    "Here, we plot the gmean of the output gene (mRuby2) as a function\n",
    "of target site number for the OL and CL ComMAND circuits. Note that\n",
    "this plot would make more sense with additional biological replicates.\n",
    "'''\n",
    "# Add plasmid metadata to stats\n",
    "stats = stats.merge(metadata, how='left', on='construct')\n",
    "\n",
    "# Plot mRuby2 geometric mean for each condition\n",
    "plot_df = stats[stats['ts_kind']!='na']\n",
    "ax = sns.stripplot(\n",
    "        data=plot_df, x='ts_num', y='mRuby2-A_gmean', \n",
    "        hue='ts_kind', palette={'NT': 'gray', 'T': 'teal'},\n",
    "        size=10, jitter=False  # arguments specific to stripplot: the size of the markers and the amount of random x-offset\n",
    "    )\n",
    "ax.set(yscale='log', xlabel='# of target sites', ylabel='ouput (gmean)')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 2: Small molecule titration experiment\n",
    "\n",
    "Here, we load another dataset, where varying the concentration\n",
    "of a small molecule (auxin) changes the expression of EGFP.\n",
    "'''\n",
    "# Load another dataset: Emma's auxin calibration curve\n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'Emma'/'2022.03.12_Auxin_Calib'/'Data'\n",
    "data5 = rd.flow.load_csv_with_metadata(data_path, data_path/'wells.yaml', columns=['EGFP-A'])\n",
    "data5 = data5[data5['EGFP-A']>0]\n",
    "display(data5)\n",
    "\n",
    "# Compute EGFP gmean for each replicate of each auxin concentration, excluding untransfected cells (NT)\n",
    "stats5 = data5[data5.Auxin!='NT'].groupby(['Auxin','Replicates'])['EGFP-A'].apply(sp.stats.gmean).rename('EGFP-A_gmean').reset_index()\n",
    "display(stats5)\n",
    "\n",
    "# Plot summary statistics\n",
    "plot_df = stats5[stats5.Auxin > 0] # exclude Auxin = 0 condition for more convenient plotting\n",
    "ax = sns.scatterplot(\n",
    "        data=plot_df, x='Auxin', y='EGFP-A_gmean', \n",
    "        hue='Auxin', palette=no_yellow_viridis, hue_norm=matplotlib.colors.LogNorm(), # distribute viridis colors in logspace\n",
    "        legend=False  # exclude the legend, since colors are the same as the x-values\n",
    "    )\n",
    "ax.set(xscale='log',yscale='log', xlim=(1e-1,1e3)) # note: use xscale='symlog' to include the Auxin=0 condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 3: Alternative ways to plot replicates\n",
    "\n",
    "Using the same data as above, we can collapse replicates into\n",
    "a single value with an estimate of their spread. Notice that \n",
    "these functions will summarize the replicates for you, without\n",
    "requiring any additional calculations.\n",
    "'''\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,5), sharey=True)\n",
    "plot_df = stats5\n",
    "\n",
    "# Plot as line with shaded region\n",
    "sns.lineplot(\n",
    "        data=plot_df, x='Auxin', y='EGFP-A_gmean', \n",
    "        estimator='median', errorbar='ci',   # arguments specific to lineplot\n",
    "        ax=axes[0]  # since we created a figure above, pass the axis to plot on\n",
    "    )\n",
    "\n",
    "# Plot as points with error bars\n",
    "sns.lineplot(\n",
    "        data=plot_df, x='Auxin', y='EGFP-A_gmean',\n",
    "        estimator='median', errorbar='ci',    # plot the same stats as above (median + confidence interval)\n",
    "        err_style='bars', marker='o', ls='',  # instead of a line, plot individual points with errorbars\n",
    "        ax=axes[1],\n",
    "    )\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set(xscale='log', yscale='log', ylim=(1e2,1e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional computations\n",
    "\n",
    "Besides simple summary statistics, you may be interested in computing metrics like fraction positive in a particular channel or the fold change of one condition relative to another. Below are a few metrics that might be useful, or that might give you ideas for approaching other calculations. Note that there are several ways to perform these calculations; these are each just one approach.\n",
    "\n",
    "The more you learn about the Pandas package, the better you can leverage built-in functions to help you perform computations more efficiently. The \"[split-apply-combine](https://pandas.pydata.org/docs/user_guide/groupby.html)\" framework is often a helpful strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gated fraction\n",
    "\n",
    "For a given channel, calculate the fraction of cells in each\n",
    "condition that have values greater than the specified threshold.\n",
    "Essentially, divide the count of gated cells by the count of ungated\n",
    "cells in each group. Then, clean up the resulting DataFrame so the\n",
    "index and columns are easy to read.\n",
    "'''\n",
    "fraction = (data_gated.groupby('construct')['mGL-A'].count() / \n",
    "            data.groupby('construct')['mGL-A'].count()).reset_index().rename(columns={'mGL-A': 'fraction'}).dropna()\n",
    "display(fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fold change\n",
    "\n",
    "Compute the fold change of one statistic for some condition,\n",
    "or set of conditions, relative to some baseline condition.\n",
    "Here, we find the fold change of the output (mRuby2) for OL\n",
    "and CL circuits relative to their respective 1x target site \n",
    "conditions. The new function defined takes DataFrame groups, \n",
    "computes the baseline value for that group (output gmean of 1x\n",
    "target site condition), and returns the original DataFrame group\n",
    "with an added column containing the computed fold change.\n",
    "'''\n",
    "# Define a function to compute fold change within a group\n",
    "def get_fc(df):\n",
    "    d = df.copy()\n",
    "    baseline = d.loc[d['ts_num']==1, 'mRuby2-A_gmean'].mean()\n",
    "    d['fold_change'] = d['mRuby2-A_gmean'] / baseline\n",
    "    return d\n",
    "\n",
    "# Compute fold change within each circuit type (base/OL/CL defined by 'ts_kind')\n",
    "stats = stats.groupby('ts_kind')[stats.columns].apply(get_fc).reset_index(drop=True)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Quantile binning\n",
    "\n",
    "Rather than calculating summary statistics on an entire\n",
    "condition, bin the data into equal-quantile groups based \n",
    "on values of a given channel (e.g., 10 bins each with 10% \n",
    "of the data). Here, we bin on the transfection marker (mGL).\n",
    "'''\n",
    "# Assign quantiles\n",
    "num_bins = 20\n",
    "data['bin_quantiles'] = data.groupby('construct')['mGL-A'].transform(lambda x: pd.qcut(x, q=num_bins, duplicates='drop'))\n",
    "\n",
    "# Calculate the median of each bin\n",
    "quantiles = data.groupby(['construct','bin_quantiles'])['mGL-A'].median().rename('bin_quantiles_median').reset_index()\n",
    "\n",
    "# Create a new column in data with the bin median\n",
    "data = data.merge(quantiles, how='left', on=['construct','bin_quantiles'])\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "Quadrants defined by two gates\n",
    "\n",
    "Categorize cells into quadrants based on two gates/channels.\n",
    "The function defined below takes the two columns to gate on\n",
    "plus their gates, then returns a DataFrame column (Series) with\n",
    "the quadrant number.\n",
    "Possible values for 'quadrant':\n",
    "  0 = double negative\n",
    "  1 = x-positive\n",
    "  2 = y-positive\n",
    "  3 = double positive\n",
    "Then, we compute the fraction of cells in each quadrant (notice that\n",
    "this is an alternative to the earlier approach) and rename the \n",
    "quadrants with useful labels.\n",
    "'''\n",
    "def get_quadrant(x, y, gate_x, gate_y):\n",
    "    df_quad = pd.DataFrame()\n",
    "    df_quad['x'] = x > gate_x\n",
    "    df_quad['y'] = y > gate_y\n",
    "    df_quad['quadrant'] = df_quad['x'].astype(int) + df_quad['y'].astype(int)*2\n",
    "    return df_quad['quadrant']\n",
    "\n",
    "# Categorize each cell into a quadrant\n",
    "gate_mRuby2 = data.loc[data.construct=='UT', 'mRuby2-A'].quantile(0.999)\n",
    "data['quadrant'] = get_quadrant(data['mGL-A'], data['mRuby2-A'], gate, gate_mRuby2)\n",
    "\n",
    "# Compute fraction of cells in each quadrant\n",
    "quadrants = data.groupby(['construct','quadrant'])['mGL-A'].count().rename('count')\n",
    "quadrants = (quadrants/quadrants.groupby('construct').transform('sum')).dropna().reset_index(name='fraction')\n",
    "\n",
    "# Rename quadrant numbers with interpretable labels\n",
    "quadrants['label'] = quadrants.quadrant.map({0: 'double-negative', 1: 'mGL-positive', 2: 'mRuby2-positive', 3: 'double-positive'})\n",
    "display(quadrants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fitting to a model\n",
    "\n",
    "Here, we use one of the earlier datasets where Emma has generated a calibration \n",
    "curve for auxin (a small molecule that leads to degradation of proteins with the \n",
    "associated AID tag) by varying the auxin concentration and measuring the \n",
    "resulting drop in EGFP-AID fluorescence. From the literature, we find an \n",
    "equation that explains this relationship (described in 'my_model') and fit the \n",
    "coefficients using scipy's 'curve_fit' function. Then, we plot the results.\n",
    "'''\n",
    "# Define a function for the model, where x is auxin concentration in µM\n",
    "#  and the result is log10(fluorescence)\n",
    "def my_model(x, basal_fluorescence, amplitude, EC50):\n",
    "    return basal_fluorescence - amplitude * x/(x+EC50)\n",
    "\n",
    "# Fit the data to the model and print the results\n",
    "fit_df = stats5[stats5.Auxin > 0] # ignore the Auxin=0 condition for easier plotting\n",
    "popt, pcov = sp.optimize.curve_fit(         # returns the optimized parameters ('popt') and \n",
    "        my_model,                           # the function to fit to\n",
    "        fit_df.Auxin,                       # the x-values\n",
    "        np.log10(fit_df['EGFP-A_gmean'])    # the y-values\n",
    "    )\n",
    "\n",
    "# Print 'popt', the optimized values for the other arguments in 'my_model'\n",
    "#  and 'pcov', the estimated covariance of 'popt' \n",
    "perr = np.sqrt(np.diag(pcov)) # gives one standard deviation error for the parameter estimates\n",
    "print(f'basal fluorescence (log10): {popt[0]:.1f} -/+ {perr[0]:.1f}')\n",
    "print(f'amplitude: {popt[1]:.1f} -/+ {perr[1]:.1f}')\n",
    "print(f'EC50 (µM): {popt[2]:.1f} -/+ {perr[2]:.1f}')\n",
    "\n",
    "# Plot the data\n",
    "ax = sns.scatterplot(data=fit_df, x='Auxin', y='EGFP-A_gmean',)\n",
    "ax.set(xscale='log', yscale='log')\n",
    "\n",
    "# Plot the model fit on the same axes\n",
    "xs = np.logspace(np.log10(fit_df.Auxin.min()), np.log10(fit_df.Auxin.max()), 1000)\n",
    "ys = my_model(xs, *popt)\n",
    "sns.lineplot(x=xs, y=10**ys)\n",
    "\n",
    "# Include EC_50 info on the plot\n",
    "ax.axvline(popt[2], color='gray', zorder=0)\n",
    "ax.axvspan(popt[2]-perr[2], popt[2]+perr[2], facecolor='gray', alpha=0.2, edgecolor=None)\n",
    "ax.annotate(\n",
    "        r'EC$_{50}$ = ' + f'{popt[2]:.1f} µM',  # the text to add (use an r-string to read latex)\n",
    "        (0.05, 0.05), xycoords='axes fraction', # the location of the text, in fraction of the axes (from 0 to 1)\n",
    "        color='gray'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now go forth and explore your data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3236d7b25a7341c21bf01a429cde5058fcb015209b90ff3de21773f23af396f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
