{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all our favorite packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rushd as rd\n",
    "import scipy as sp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the path to the SharePoint using `datadir`\n",
    "To enable reproducible analysis, Python notebooks/code should be able to run from locations other than your personal computer. However, because our data is stored in the cloud, the location of the data files will differ across users. Namely, accessing the lab SharePoint requires an absolute path specific to your computer. \n",
    "\n",
    "To get around this issue, `rushd` facilitates one-time user specification of the path to this data directory, or `datadir`, that remains constant for a particular location (i.e., a github repository stored on your computer). This way, you include the absolute path to the data-containing directory outside of your Python notebook, meaning that cloning the repository / running the code elsewhere requires only one change to properly run the analyses.\n",
    "\n",
    "To specify a `datadir`, write the absolute path to the data-containing directory in a text file called `datadir.txt`. Use the highest-level directory you anticipate needing, i.e., the main SharePoint (not one of the subfolders). Include only this line in the file, and do not enclose the path in quotes. For instance, the path to the SharePoint on my computer is: \n",
    "\n",
    "`/Users/kaseylove/Massachusetts Institute of Technology/GallowayLab - Documents`\n",
    "\n",
    "so my `datadir.txt` file contains just this line. The file is stored in the root directory of my git repository.\n",
    "\n",
    "Then, to access data in this directory, simply use `rd.datadir` as the path to this folder. To access subfolders, normal Path-type operations apply, e.g., `rd.datadir/'subfolder'`. Additional usage examples are below. \n",
    "\n",
    "The path to root directory of your repository is also conveniently accessible via `rushd` using `rd.rootdir` (no text file required). This is helpful for specifying output paths when saving figures/files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add well metadata using a `.yaml` file\n",
    "\n",
    "The default Attune and FlowJo filenames for samples comprise the well number without other condition information. To add this metadata (e.g., plasmids, small molecules), create a `.yaml` file to map conditions to wells. See this quick tutorial for creating these files: https://learnxinyminutes.com/docs/yaml/. Here is an example format:\n",
    "\n",
    "```\n",
    "metadata:\n",
    "  key1:\n",
    "    - valueA: A1-A12\n",
    "    - valueB: B1-B12\n",
    "  key2:\n",
    "    - 0: A1-H1\n",
    "    - 1: A2-H2\n",
    "```\n",
    "\n",
    "Save this file in the same folder as (or near) your raw data.\n",
    "\n",
    "To double check that the `.yaml` file correctly specifies conditions, you can display the associated plate map using `rushd`.\n",
    "See the example below and https://gallowaylabmit.github.io/rushd/en/main/tutorial/plot_well_metadata.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plate layout example\n",
    "\n",
    "View the layout of conditions on an example plate.\n",
    "'''\n",
    "yaml_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.01.24_exp80.3'/'export'/'exp80.3_wells.yaml'\n",
    "rd.plot.plot_well_metadata(yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: Use `rushd` package to import `.csv` data into a Pandas DataFrame with associated metadata\n",
    "The function loads data from all wells into a single DataFrame object, which is nicely compatible with Seaborn plotting functions (see description of 'long-form' data here: https://seaborn.pydata.org/tutorial/data_structure.html and Pandas documentation here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "Each row is a single cell, and columns are the data associated with that cell. This includes all the measurement channels on the Attune (e.g., FSC-H, VL1-A, mCherry-A, Time -- it will use the names that you set for the channels on the Attune software) as well as the information from the filename (e.g., well, FlowJo population) and your `.yaml` file (e.g., plasmid/construct, small molecule conditions, replicate number).\n",
    "\n",
    "See examples below. Feel free to take a look at the filenames and `.yaml` files in the indicated folder for reference. If your `datadir` is set up properly, you should be able to run these cells and see the data loaded into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 1: Single plate\n",
    "\n",
    "This uses the default filename from exporting in FlowJo, namely `export_{well}_{population}.csv`.\n",
    "Columns labeled 'well' and 'population' are added based on the filename.\n",
    "Here, for instance, the file `export_A1_singlets.csv` is added with 'A1' in the 'wells' column \n",
    "and 'singlets' in the 'population' column.\n",
    "'''\n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.12.04_exp092.3'/'export'\n",
    "yaml_path = data_path/'wells.yaml'\n",
    "data = rd.flow.load_csv_with_metadata(data_path, yaml_path)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 2: Load selected columns\n",
    "\n",
    "This example loads the same data as above, but only the two channels we care about,\n",
    "specified via the 'columns' argument. This saves time/space by not storing values \n",
    "for irrelevant Attune channels.\n",
    "'''\n",
    "channel_list = ['mRuby2-A','mGL-A']\n",
    "data = rd.flow.load_csv_with_metadata(data_path, yaml_path, columns=channel_list)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 3: Multiple plates\n",
    "\n",
    "This example loads four plates. Files are named with the default FlowJo naming as above, and\n",
    "the data for each plate is stored in separate folders. This data is then loaded into a single\n",
    "DataFrame with extra metadata specifying the cell type in each plate.\n",
    "'''\n",
    "base_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.02.07_exp77.3'/'export'\n",
    "\n",
    "plates = pd.DataFrame({\n",
    "    'data_path': [base_path/f'plate{n}' for n in range(1,5)],\n",
    "    'yaml_path': [base_path/'exp77.3_plate1_wells.yaml', base_path/'exp77.3_plate2_wells.yaml',]*2,\n",
    "    'cell': ['MEF', 'MEF', '293T', '293T'],\n",
    "})\n",
    "\n",
    "data2 = rd.flow.load_groups_with_metadata(plates)\n",
    "display(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to a local cache\n",
    "\n",
    "Loading data from the SharePoint can sometimes take several minutes since each file must be downloaded from the server. To speed up this step for future analyses, you can download the data once from the SharePoint and save the relevant components locally on your computer. It is useful to combine this step with the original loading one.\n",
    "\n",
    "A convenient place for this cache is in a folder in the analysis repo. I like to save this file in the same place as any plots I generate during analysis: a folder for the experiment in the `output` folder of my git repo. Make sure to add the `output` folder to `.gitignore` so that git doesn't track these large files.\\\n",
    "\n",
    "`rushd` also provides the function `outfile` that creates a `.yaml` file containing metadata associated with the output (e.g., git version). It will also create the directories in the path if they don't already exist, which is convenient. You can wrap any output path in `outfile`, including your data cache and any plots you generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = rd.rootdir/'output'/'exp092.3'/'data.gzip'\n",
    "\n",
    "# Specify the data to load\n",
    "channel_list = ['mRuby2-A','mGL-A']\n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.12.04_exp092.3'/'export'\n",
    "yaml_path = data_path/'wells.yaml'\n",
    "\n",
    "data3 = pd.DataFrame()\n",
    "\n",
    "# If cache exists, load data from cache\n",
    "if cache_path.is_file(): \n",
    "    data3 = pd.read_parquet(cache_path)\n",
    "\n",
    "# Otherwise, load from SharePoint and create cache\n",
    "else: \n",
    "    data3 = rd.flow.load_csv_with_metadata(data_path, yaml_path, columns=channel_list)\n",
    "    data3.to_parquet(rd.outfile(cache_path))\n",
    "    \n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional condition-level metadata\n",
    "\n",
    "Sometimes conditions, such as plasmids, vary across multiple dimensions (promoter, gene, syntax, etc.) that would be helpful to add as metadata. This can be cumbersome to add to your `.yaml` file, especially if you have multiple plasmids per condition or reuse plasmids across experiments. Therefore, it can be helpful to include only the plasmid name in the `.yaml` file mapping conditions to wells and also to create a single spreadsheet with plasmid metadata. You can save this in the git repo directly, or in a project folder in the SharePoint.\n",
    "\n",
    "For example, a spreadsheet for TANGLES constructs could look like this:\n",
    "\n",
    "| plasmid | upstream_gene | downstream_gene | spacer | syntax             |\n",
    "| ------- | ------------- | --------------- | ------ | ------------------ |\n",
    "| pTA001  | tagBFP        | mRuby2          | 1x     | downstream_tandem  |\n",
    "| pTA002  | tagBFP        | mRuby2          | 1x     | divergent          |\n",
    "| pTA003  | tagBFP        | mRuby2          | 1x     | convergent         |\n",
    "| pTA004  | mRuby2        | tagBFP          | 1x     | upstream_tandem    |\n",
    "\n",
    "You can then load this spreadsheet as a DataFrame and merge it with your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 1: Column names match\n",
    "\n",
    "If the name of the first column in the metadata file matches a column name in your data, \n",
    "you can merge directly using the 'on' argument.\n",
    "'''\n",
    "metadata_path = rd.datadir/'projects'/'miR-iFFL'/'plasmids'/'construct-metadata.xlsx'\n",
    "metadata = pd.read_excel(metadata_path)\n",
    "data = data.merge(metadata, how='left', on='construct')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 2: Multiple columns with plasmids\n",
    "\n",
    "If you need to add metadata information for two columns in your data, you can repeatedly\n",
    "call `merge` on each. See the pandas documentation for more information. This example \n",
    "adds plasmid metadata to the 'construct' column (the reporter plasmid) and the 'activator' \n",
    "column (co-transfected activator plasmid). Notice that the activator metadata contain a\n",
    "suffix to differentiate them.\n",
    "'''\n",
    "\n",
    "# Load data \n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'kasey'/'2024.07.16_exp099'/'export_comp'\n",
    "yaml_path = data_path/'wells.yaml'\n",
    "cache_path = rd.rootdir/'output'/'KL_exp099'/'data.gzip'\n",
    "\n",
    "data3 = pd.DataFrame()\n",
    "if cache_path.is_file(): data3 = pd.read_parquet(cache_path)\n",
    "else: \n",
    "    channel_list = ['mRuby2-A','AF514-A','tagBFP-A']\n",
    "    data3 = rd.flow.load_csv_with_metadata(data_path, yaml_path, columns=channel_list)\n",
    "    data3.to_parquet(rd.outfile(cache_path))\n",
    "\n",
    "# Add metadata\n",
    "metadata3 = pd.read_excel(rd.datadir/'projects'/'geec'/'construct-metadata_KL.xlsx')\n",
    "data3 = data3.merge(metadata3, how='left', on='construct')\n",
    "data3 = data3.merge(metadata3, how='left', right_on='construct', left_on='activator', suffixes=(None,'_activator'))\n",
    "display(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have your data loaded with useful metadata! It's time to see what the data shows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set plotting defaults\n",
    "\n",
    "To explore trends in your data, you'll make a bunch of plots. To make the plots look nicer, you can set some basic defaults for font size, line width, etc. (This is much more important for polished figures, but starting with decent plots now will make even quick slides easier to understand.) \n",
    "\n",
    "This is also a good time to define a set color palette. See Seaborn \"Choosing color palettes\" for suggestions, or try a palette from someone else in lab. You can specify colors using the matplotlib named colors, hex codes, or a few other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seaborn style (applies to entire notebook)\n",
    "\n",
    "The 'talk' context sets font size, etc. appropriate for a presentation.\n",
    "(Other options include 'notebook', 'paper', and 'poster'.)\n",
    "I also set the font to Helvetica Neue, but you can change this to whatever you prefer.\n",
    "See Seaborn/matplotlib documentation for other parameters.\n",
    "'''\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('talk', rc={'font.family': 'sans-serif', 'font.sans-serif':['Helvetica Neue']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a color palette\n",
    "\n",
    "Use a dictionary to map categorical condition values to colors. \n",
    "This modified form of the viridis palette (yellow -> purple) is \n",
    "good for continuous values (e.g., small molecule amounts).\n",
    "'''\n",
    "palette = {\n",
    "    'tandem_reporter_upstream': '#225A9B',\n",
    "    'tandem_reporter_downstream': '#19D2BF',\n",
    "    'convergent': '#FFB133',\n",
    "    'divergent': '#FE484E',\n",
    "}\n",
    "\n",
    "no_yellow_viridis = matplotlib.colors.ListedColormap(matplotlib.colormaps['viridis'](np.linspace(0, 0.85, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove cells with negative channel values\n",
    "\n",
    "Negative values from the Attune are essentially \"off the chart\" and represent non-expressing cells. There aren't usually too many of them, and it is safe to simply exclude them. This makes it simpler to plot the data, which is log-distributed.\n",
    "\n",
    "To remove these cells, you can use a \"mask\", finding the rows that satisfy some True/False statement and then reassigning `data` to this value. To remove cells with negative channel measurements, the statement will specify that the value in each channel column be greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove cells with negative channel values\n",
    "\n",
    "The 'channel_list' should contain all the Attune channels you're \n",
    "interested in (e.g., mGL-A).\n",
    "'''\n",
    "for c in ['mRuby2-A','mGL-A']:\n",
    "    data = data[data[c] > 0]\n",
    "    data2 = data2[data2[c] > 0]\n",
    "\n",
    "channel_list = ['mRuby2-A','AF514-A','tagBFP-A']\n",
    "for c in channel_list:\n",
    "    data3 = data3[data3[c] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms & joint distributions\n",
    "\n",
    "The first thing you'll likely want to do is visualize distributions of expression across relevant channels for various conditions. The easiest way to plot multiple conditions at once is by using Seaborn's FacetGrid and related functions: https://seaborn.pydata.org/tutorial/axis_grids.html With these functions, you shouldn't have to loop over manual axes!\n",
    "\n",
    "Use `kdeplot` to plot 1D or 2D distributions: https://seaborn.pydata.org/generated/seaborn.kdeplot.html Note that 2D kdeplots may take several minutes to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 1: Plot 1D kdeplot\n",
    "\n",
    "For kdeplots, be sure to use a log scale and normalize the area under the\n",
    "curve within conditions rather than across them (no \"common normalization\",\n",
    "i.e., 'common_norm=False').\n",
    "You can manually adjust the placement of the legend to move it outside of\n",
    "the plot area.\n",
    "'''\n",
    "ax = sns.kdeplot(data=data, x='mGL-A', hue='construct',\n",
    "                log_scale=True, common_norm=False)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 2: Plot kdeplots in a grid\n",
    "\n",
    "Use the corresponding grid functions to facet your data along \n",
    "rows/columns of a grid. \n",
    "Here, we subset the data to only plot conditions where the \n",
    "'ts_kind' column has the value 'NT' (OL circuit) or 'T' (CL circuit).\n",
    "'''\n",
    "plot_df = data[data.ts_kind.isin(['NT','T'])]\n",
    "g = sns.displot(data=plot_df, x='mGL-A', hue='ts_num', palette=no_yellow_viridis,\n",
    "                col='ts_kind',\n",
    "                log_scale=True, common_norm=False, kind='kde', facet_kws=dict(margin_titles=True))\n",
    "\n",
    "# Loop over the axes to add the untransfected condition for comparison\n",
    "for ax in g.axes_dict.values():\n",
    "    sns.kdeplot(data=data[data.construct=='UT'], x='mGL-A', color='black', ls=':', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 2: Plot 2D kdeplot in a grid\n",
    "\n",
    "It can help to downsample your data to fewer cells per condition\n",
    "so that initial plots generate more quickly (remove this for \n",
    "final figures).\n",
    "'''\n",
    "plot_df = data3[(data3.gene_activator!='na') & (data3.promoter_activator!='na')].groupby(['construct','activator']).sample(1000)\n",
    "g = sns.displot(data=plot_df, x='AF514-A', y='mRuby2-A', hue='inducer',\n",
    "                row='gene_activator', col='promoter_activator',\n",
    "                log_scale=True, common_norm=False, kind='kde', facet_kws=dict(margin_titles=True))\n",
    "g.set(xlim=(1e1,1e5), ylim=(1e0,1e5))\n",
    "g.set_titles(row_template='{row_name}', col_template='activator promoter: {col_name}')\n",
    "g.refline(y=2e2, color='black', ls='-', zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate expressing cells\n",
    "\n",
    "Depending on your experiment, you might want to analyze only a fraction of the population. For transfections, we typically only care about transfected cells, or those expressing the transfection marker (co-delivered fluorescent protein). You can manually eyeball this threshold (or gate), or you can set it based some high percentile of the untransfected cells. It can be helpful to make a new DataFrame with only these cells. Be sure to exclude any conditions that lack the transfection marker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 1: Choose a threshold manually\n",
    "'''\n",
    "gate = 3e2\n",
    "data_gated = data[(data['mGL-A']>gate) & (data.construct!='UT')].copy()\n",
    "\n",
    "ax = sns.kdeplot(data=data, x='mGL-A', hue='construct', \n",
    "                 log_scale=True, common_norm=False)\n",
    "ax.axvline(gate, color='black', zorder=0)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example 2: Gate based on the untransfected population\n",
    "\n",
    "Here, we use the 99.9th percentile of the untransfected population\n",
    "as the gate, meaning that only 0.1% of untransfected cells are (mis)labeled\n",
    "as expressing.\n",
    "'''\n",
    "gate = data.loc[data.construct=='UT', 'mGL-A'].quantile(0.999)\n",
    "display(gate)\n",
    "data_gated = data[(data['mGL-A']>gate) & (data.construct!='UT')].copy()\n",
    "\n",
    "ax = sns.kdeplot(data=data, x='mGL-A', hue='construct', \n",
    "                 log_scale=True, common_norm=False)\n",
    "ax.axvline(gate, color='black', zorder=0)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 3: Use different gates for different conditions\n",
    "\n",
    "We want to use different gates for the MEFs and 293T cells since they have\n",
    "different autofluorescence profiles. We'll define gates based on the \n",
    "uninfected ('UI') populations for each cell type.\n",
    "'''\n",
    "# Define a function to choose the gate and return the gated population\n",
    "def gate_data(df):\n",
    "    gate = df.loc[df.construct=='UI', 'mGL-A'].quantile(0.999)\n",
    "    display(gate)\n",
    "    return data[(data['mGL-A']>gate) & (data.construct!='UI')]\n",
    "\n",
    "# Gate data\n",
    "data2_gated = data2.groupby(['cell'])[data2.columns].apply(gate_data).reset_index()\n",
    "display(data2_gated)\n",
    "\n",
    "# Plot uninfected populations to visualize autofluorescence profiles\n",
    "plot_df = data2[data2.construct=='UI'].groupby('cell').sample(1000)\n",
    "cell_palette = {'293T': 'teal', 'MEF': 'orange'}\n",
    "g = sns.displot(data=plot_df, x='mGL-A', y='mRuby2-A', col='cell', \n",
    "                hue='cell', palette=cell_palette,\n",
    "                kind='kde', log_scale=True)\n",
    "g.refline(x=243, ls='-', color=cell_palette['293T'], zorder=0)\n",
    "g.refline(x=335, ls='-', color=cell_palette['MEF'], zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate summary statistics\n",
    "\n",
    "Now that you've explored the distributions for each condition, you probably want to quantify trends. Calculating summary statistics (mean, standard deviation, etc.) is straightforward and quick with Pandas functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate summary statistics for multiple channels at once\n",
    "\n",
    "We want to use different gates for the MEFs and 293T cells since they have\n",
    "different autofluorescence profiles.\n",
    "'''\n",
    "# Compute geometric mean (gmean) and standard deviation on two relevant channels\n",
    "channel_list = ['mGL-A','mRuby2-A']\n",
    "stats = data_gated.groupby('construct')[channel_list].agg([sp.stats.gmean, np.std]).reset_index().dropna()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "stats.columns = ['_'.join(c).rstrip('_') for c in stats.columns.to_flat_index()]\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "A function for computing summary statistics\n",
    "\n",
    "I turned the computation above into a convenient function. \n",
    "Maybe soon we'll add it to rushd, but for now feel free to\n",
    "use & modify it yourself!\n",
    "\n",
    "Inputs\n",
    "  df: your DataFrame\n",
    "  by: a list of columns used to group the data for summarizing\n",
    "  columns: a list of columns to summarize\n",
    "  stats: a list of functions to use to summarize\n",
    "'''\n",
    "def summarize(df, by, columns, stats):\n",
    "    stats = df.groupby(by)[columns].agg(stats).reset_index().dropna()\n",
    "    stats.columns = ['_'.join(c).rstrip('_') for c in stats.columns.to_flat_index()]\n",
    "    return stats\n",
    "\n",
    "stats = summarize(data_gated, 'construct', channel_list, [sp.stats.gmean, np.std])\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot summary statistics\n",
    "\n",
    "There are many ways to plot summary statistics (box plot, scatter plot, bar plot, etc.) and several ways to display the variability between measurements (error bars, shading, etc.). Choose your favorite representation!\n",
    "\n",
    "One recommendation: do not use bar plots for values without a relevant zero, and always display the zero on the axis. This ensures the sizes of the bars accurately reflect the relative values they represent. For example, bar plots are effective for displaying percentages (e.g., reprogramming purity) but not geometric mean fluorescence values (which are log-distributed and typically much higher than 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 1: Dataset 1\n",
    "\n",
    "Here, we plot the gmean of the output gene (mRuby2) as a function\n",
    "of target site number for the OL and CL ComMAND circuits. Note that\n",
    "this plot would make more sense with additional biological replicates.\n",
    "'''\n",
    "# Add plasmid metadata to stats\n",
    "stats = stats.merge(metadata, how='left', on='construct')\n",
    "\n",
    "# Plot mRuby2 geometric mean for each condition\n",
    "plot_df = stats[stats['ts_kind']!='na']\n",
    "ax = sns.stripplot(data=plot_df, x='ts_num', y='mRuby2-A_gmean', \n",
    "                   hue='ts_kind', palette={'NT': 'gray', 'T': 'teal'},\n",
    "                   size=10, jitter=False)\n",
    "ax.set(yscale='log', xlabel='# of target sites', ylabel='ouput (gmean)')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 2: Small molecule titration experiment\n",
    "\n",
    "Here, we load another dataset, where varying the concentration\n",
    "of a small molecule (auxin) changes the expression of EGFP.\n",
    "'''\n",
    "# Load another dataset: Emma's auxin calibration curve\n",
    "data_path = rd.datadir/'instruments'/'data'/'attune'/'Emma'/'2022.03.12_Auxin_Calib'/'Data'\n",
    "data4 = rd.flow.load_csv_with_metadata(data_path, data_path/'wells.yaml', columns=['EGFP-A'])\n",
    "data4 = data4[data4['EGFP-A']>0]\n",
    "display(data4)\n",
    "\n",
    "# Compute gmean for each auxin concentration, excluding untransfected cells (NT)\n",
    "stats4 = data4[data4.Auxin!='NT'].groupby(['Auxin','Replicates'])['EGFP-A'].apply(sp.stats.gmean).rename('EGFP-A_gmean').reset_index()\n",
    "\n",
    "# Plot summary statistics\n",
    "plot_df = stats4[stats4.Auxin > 0]\n",
    "ax = sns.scatterplot(data=plot_df, x='Auxin', y='EGFP-A_gmean', \n",
    "                     hue='Auxin', palette=no_yellow_viridis, hue_norm=matplotlib.colors.LogNorm(),\n",
    "                     legend=False)\n",
    "ax.set_xscale('symlog', linthresh=0.5)\n",
    "ax.set(yscale='log', xlim=(0.4,1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 3: Alternative ways to plot replicates\n",
    "\n",
    "Using the same data as above, we can collapse replicates into\n",
    "a single value with an estimate of their spread. Notice that \n",
    "these functions will summarize the replicates for you, without\n",
    "requiring any additional calculations.\n",
    "'''\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,5), sharey=True)\n",
    "plot_df = stats4\n",
    "\n",
    "# Plot as line with shaded region\n",
    "sns.lineplot(data=plot_df, x='Auxin', y='EGFP-A_gmean', ax=axes[0],\n",
    "             estimator='median', errorbar='ci')\n",
    "\n",
    "# Plot as points with error bars\n",
    "sns.lineplot(data=plot_df, x='Auxin', y='EGFP-A_gmean', ax=axes[1],\n",
    "             estimator='median', errorbar='ci', err_style='bars',\n",
    "             marker='o', ls='')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set(xscale='log', yscale='log', ylim=(1e2,1e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional computations\n",
    "\n",
    "Besides simple summary statistics, you may be interested in computing metrics like fraction positive in a particular channel or the fold change of one condition relative to another. Below are a few metrics that might be useful, or that might give you ideas for approaching other calculations. Note that there are several ways to perform these calculations; these are each just one approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gated fraction\n",
    "\n",
    "For a given channel, calculate the fraction of cells in each\n",
    "condition that have values greater than the specified threshold.\n",
    "'''\n",
    "fraction = (data_gated.groupby('construct')['mGL-A'].count() / \n",
    "            data.groupby('construct')['mGL-A'].count()).reset_index().rename(columns={'mGL-A': 'fraction'}).dropna()\n",
    "display(fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fold change\n",
    "\n",
    "Compute the fold change of one statistic for some condition,\n",
    "or set of conditions, relative to some baseline condition.\n",
    "Here, we find the fold change of the output (mRuby2) for OL\n",
    "and CL circuits relative to their respective 1x target site \n",
    "conditions.\n",
    "'''\n",
    "# Define a function to compute fold change within a group\n",
    "def get_fc(df):\n",
    "    d = df.copy()\n",
    "    baseline = d.loc[d['ts_num']==1, 'mRuby2-A_gmean'].mean()\n",
    "    d['fold_change'] = d['mRuby2-A_gmean'] / baseline\n",
    "    return d\n",
    "\n",
    "stats = stats.groupby(by=['ts_kind'])[stats.columns].apply(get_fc).reset_index(drop=True)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Quantile binning\n",
    "\n",
    "Rather than calculating summary statistics on an entire\n",
    "condition, bin the data into equal-quantile groups based \n",
    "on values of a given channel (e.g., 10 bins each with 10% \n",
    "of the data). Here, we bin on the transfection marker (mGL).\n",
    "'''\n",
    "# Assign quantiles\n",
    "num_bins = 20\n",
    "data['bin_quantiles'] = data.groupby('construct')['mGL-A'].transform(lambda x: pd.qcut(x, q=num_bins, duplicates='drop'))\n",
    "\n",
    "# Calculate that median of each bin\n",
    "quantiles = data.groupby(['construct','bin_quantiles'])['mGL-A'].median().rename('bin_quantiles_median').reset_index()\n",
    "\n",
    "# Create a new column in data with the bin median\n",
    "data = data.merge(quantiles, how='left', on=['construct','bin_quantiles'])\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "Quadrants defined by two gates\n",
    "\n",
    "Categorize cells into quadrants based on two gates/channels.\n",
    "Possible values:\n",
    "  0 = double negative\n",
    "  1 = x-positive\n",
    "  2 = y-positive\n",
    "  3 = double positive\n",
    "Then, compute the fraction of cells in each quadrant and rename\n",
    "the quadrants with useful labels.\n",
    "'''\n",
    "def get_quadrant(x,y,gate_x,gate_y):\n",
    "    df_quad = pd.DataFrame()\n",
    "    df_quad['x'] = x > gate_x\n",
    "    df_quad['y'] = y > gate_y\n",
    "    df_quad['quadrant'] = df_quad['x'].astype(int) + df_quad['y'].astype(int)*2\n",
    "    return df_quad['quadrant']\n",
    "\n",
    "# Categorize each cell into a quadrant\n",
    "gate_mRuby2 = data.loc[data.construct=='UT', 'mRuby2-A'].quantile(0.999)\n",
    "data['quadrant'] = get_quadrant(data['mGL-A'], data['mRuby2-A'], gate, gate_mRuby2)\n",
    "\n",
    "# Compute fraction of cells in each quadrant\n",
    "quadrants = data.groupby(['construct','quadrant'])['mGL-A'].count().rename('count')\n",
    "quadrants = (quadrants/quadrants.groupby('construct').transform('sum')).dropna().reset_index(name='fraction')\n",
    "\n",
    "# Rename quadrant numbers with interpretable labels\n",
    "quadrants['label'] = quadrants.quadrant.map({0: 'double-negative', 1: 'mGL-positive', 2: 'mRuby2-positive', 3: 'double-positive'})\n",
    "display(quadrants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fitting to a model\n",
    "\n",
    "Here, we load a new dataset, where Emma has generated a calibration curve \n",
    "for auxin (a small molecule that leads to degradation of proteins with the \n",
    "associated AID tag) by varying the auxin concentration and measuring the \n",
    "resulting drop in EGFP-AID fluorescence. From the literature, we find an \n",
    "equation that explains this relationship and fit the coefficients using\n",
    "scipy's 'curve_fit'. Then, we plot the results.\n",
    "'''\n",
    "# Define a function for the model, where x is auxin concentration in µM\n",
    "#  and the result is log10(fluorescence)\n",
    "def my_model(x, basal_fluorescence, amplitude, EC50):\n",
    "    return basal_fluorescence - amplitude * x/(x+EC50)\n",
    "\n",
    "# Fit the data to the model and print the results\n",
    "fit_df = stats4[stats4.Auxin > 0]\n",
    "popt, pcov = sp.optimize.curve_fit(my_model, fit_df.Auxin, np.log10(fit_df['EGFP-A_gmean']))\n",
    "print('basal fluorescence (log10): {0:.1f}\\namplitude: {1:.1f}\\nEC50 (µM): {2:.1f}'.format(*popt))\n",
    "\n",
    "# Plot the data\n",
    "ax = sns.scatterplot(data=fit_df, x='Auxin', y='EGFP-A_gmean',)\n",
    "ax.set(xscale='log', yscale='log')\n",
    "\n",
    "# Plot the model fit on the same axes\n",
    "xs = np.logspace(np.log10(fit_df.Auxin.min()), np.log10(fit_df.Auxin.max()), 1000)\n",
    "ys = my_model(xs, *popt)\n",
    "sns.lineplot(x=xs, y=10**ys)\n",
    "ax.axvline(popt[2], color='gray', zorder=0)\n",
    "ax.annotate(r'EC$_{50}$ = ' + f'{popt[2]:.1f} µM', (0.05,0.05), xycoords='axes fraction', color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now go forth and explore your data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3236d7b25a7341c21bf01a429cde5058fcb015209b90ff3de21773f23af396f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
